import os
import argparse
import json
import torch
import numpy as np
from datetime import datetime
from pathlib import Path
import warnings

# ê²½ê³  ë©”ì‹œì§€ ì–µì œ
warnings.filterwarnings("ignore", category=DeprecationWarning)
os.environ['D4RL_SUPPRESS_IMPORT_ERROR'] = '1'
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

import gym
import d4rl
from agent import Bamba
from utils import set_seed, evaluate_bamba_on_environment
from d4rl_utils.trajectory_dataset import D4RLTrajectoryDataset
from torch.utils.data import DataLoader
import logging
from tqdm import tqdm

def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("--env", type=str, default="hopper")
    parser.add_argument("--dataset_type", type=str, default="medium")
    parser.add_argument("--context_length", type=int, default=5)
    parser.add_argument("--batch_size", type=int, default=256)
    parser.add_argument("--learning_rate", type=float, default=3e-4)
    parser.add_argument("--gamma", type=float, default=0.99)
    parser.add_argument("--tau", type=float, default=0.005)
    parser.add_argument("--alpha", type=float, default=2.5)
    parser.add_argument("--action_noise", type=float, default=0.2)
    parser.add_argument("--action_noise_clip", type=float, default=0.5)
    parser.add_argument("--hidden_dim", type=int, default=256)
    parser.add_argument("--beta_s", type=float, default=1.0)
    parser.add_argument("--beta_r", type=float, default=1.0)

    parser.add_argument("--d_model", type=int, default=256)
    parser.add_argument("--d_state", type=int, default=64)
    parser.add_argument("--d_conv", type=int, default=4)
    parser.add_argument("--expand", type=int, default=2)
    parser.add_argument("--n_layers", type=int, default=6)
    parser.add_argument("--drop_p", type=float, default=0.1)

    parser.add_argument("--warmup_steps", type=int, default=1000)
    parser.add_argument("--policy_freq", type=int, default=2)
    parser.add_argument("--num_epochs", type=int, default=1000)
    parser.add_argument("--eval_freq", type=int, default=10)
    parser.add_argument("--save_freq", type=int, default=50)
    parser.add_argument("--seed", type=int, default=42)
    parser.add_argument("--device", type=str, default="auto")
    parser.add_argument("--output_dir", type=str, default="./outputs")
    parser.add_argument("--resume", type=str, default=None)
    parser.add_argument("--dataset_dir", type=str, default="./datasets")

    return parser.parse_args()

def train_epoch(agent, dataloader, device):
    """í•œ ì—í¬í¬ í›ˆë ¨"""
    total_TD_loss = 0
    total_Mamba_loss = 0
    num_batches = 0
    
    for batch in dataloader:
        # GPUë¡œ ë°ì´í„° ì´ë™
        states = batch['states'].to(device)
        next_states = batch['next_states'].to(device)
        actions = batch['actions'].to(device)
        rewards = batch['rewards'].to(device)
        dones = batch['dones'].to(device)
        
        # ì—ì´ì „íŠ¸ ì—…ë°ì´íŠ¸
        TD_loss, Mamba_loss = agent.update(states, actions, rewards, next_states, dones)
        
        total_TD_loss += TD_loss
        total_Mamba_loss += Mamba_loss
        num_batches += 1
    
    # num_batchesê°€ 0ì¸ ê²½ìš° ë°©ì§€
    if num_batches == 0:
        return 0.0, 0.0
    
    return total_TD_loss / num_batches, total_Mamba_loss / num_batches

def save_checkpoint(agent, epoch, 
                   normalization_stats, output_dir, filename=None):
    """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
    if filename is None:
        filename = f"checkpoint_epoch_{epoch}.pt"
    
    checkpoint = {
        'epoch': epoch,
        'agent_state_dict': {
            'encoder': agent.encoder.state_dict(),
            'critic': agent.critic.state_dict(),
            'critic_target': agent.critic_target.state_dict(),
            'action_selector': agent.action_selector.state_dict(),
        },
        'optimizer_states': {
            'mamba_optimizer': agent.mamba_optimizer.state_dict(),
            'critic_optimizer': agent.critic_optimizer.state_dict(),
        },
        'normalization_stats': normalization_stats,
        'hyperparameters': {
            'state_dim': agent.state_dim,
            'action_dim': agent.action_dim,
            'hidden_dim': agent.hidden_dim,
            'context_length': agent.context_length,
            'd_model': agent.d_model,
            'd_state': agent.d_state,
            'd_conv': agent.d_conv,
            'expand': agent.expand,
            'n_layers': agent.n_layers,
            'warmup_steps': agent.warmup_steps,
            'tau': agent.tau,
            'alpha': agent.alpha,
            'gamma': agent.gamma,
            'action_noise': agent.action_noise,
            'action_noise_clip': agent.action_noise_clip,
            'beta_s': agent.beta_s,
            'beta_r': agent.beta_r,
        }
    }
    
    checkpoint_path = os.path.join(output_dir, filename)
    torch.save(checkpoint, checkpoint_path)
    print(f"âœ… ì²´í¬í¬ì¸íŠ¸ ì €ì¥ë¨: {checkpoint_path}")

def load_checkpoint(agent, checkpoint_path, device):
    """ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ"""
    checkpoint = torch.load(checkpoint_path, map_location=device)
    
    # ì—ì´ì „íŠ¸ ìƒíƒœ ë³µì›
    agent.encoder.load_state_dict(checkpoint['agent_state_dict']['encoder'])
    agent.critic.load_state_dict(checkpoint['agent_state_dict']['critic'])
    agent.critic_target.load_state_dict(checkpoint['agent_state_dict']['critic_target'])
    agent.action_selector.load_state_dict(checkpoint['agent_state_dict']['action_selector'])
    
    # ì˜µí‹°ë§ˆì´ì € ìƒíƒœ ë³µì›
    if 'optimizer_states' in checkpoint:
        agent.mamba_optimizer.load_state_dict(checkpoint['optimizer_states']['mamba_optimizer'])
        agent.critic_optimizer.load_state_dict(checkpoint['optimizer_states']['critic_optimizer'])
        print("âœ… ì˜µí‹°ë§ˆì´ì € ìƒíƒœ ë³µì›ë¨")
    
    # ì •ê·œí™” í†µê³„ ë³µì›
    normalization_stats = checkpoint['normalization_stats']
    
    print(f"âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë“œë¨: {checkpoint_path}")
    return checkpoint['epoch'], normalization_stats

def main():
    args = parse_args()
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    output_dir = Path(args.output_dir)
    output_dir.mkdir(exist_ok=True)
    
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(output_dir / 'training.log'),
            logging.StreamHandler()
        ]
    )
    
    # ì‹œë“œ ì„¤ì •
    set_seed(args.seed)
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    if args.device == "auto":
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    else:
        device = torch.device(args.device)
    
    print(f"ğŸš€ í›ˆë ¨ ì‹œì‘ - í™˜ê²½: {args.env}-{args.dataset_type}-v2, ë””ë°”ì´ìŠ¤: {device}")
    
    # D4RL Trajectory Dataset ì´ˆê¸°í™”
    print("ğŸ“Š D4RL Trajectory Dataset ì´ˆê¸°í™” ì¤‘...")
    dataset = D4RLTrajectoryDataset(
        env=args.env,
        dataset_type=args.dataset_type,
        dataset_dir=args.dataset_dir,
        context_length=args.context_length
    )
    
    # ë°ì´í„°ë¡œë” ìƒì„±
    dataloader = DataLoader(
        dataset, 
        batch_size=args.batch_size, 
        shuffle=True, 
        num_workers=0,  # Windows í˜¸í™˜ì„±ì„ ìœ„í•´ 0ìœ¼ë¡œ ì„¤ì •
        drop_last=True
    )
    
    # ë°ì´í„°ì…‹ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    state_mean, state_std = dataset.get_state_stats()
    normalization_stats = {
        'observation_mean': state_mean,
        'observation_std': state_std
    }
    
    print(f"ğŸ“ˆ ë°ì´í„°ì…‹ í¬ê¸°: {len(dataset)} ì‹œí€€ìŠ¤")
    print(f"ğŸ”§ ìƒíƒœ ì°¨ì›: {dataset[0]['states'].shape[1]}, ì•¡ì…˜ ì°¨ì›: {dataset[0]['actions'].shape[0]}")
    print(f"ğŸ”§ ë°ì´í„°ì…‹ í†µê³„ - í‰ê· : {state_mean}, í‘œì¤€í¸ì°¨: {state_std}")
    # ì—ì´ì „íŠ¸ ìƒì„±
    print("ğŸ¤– Bamba ì—ì´ì „íŠ¸ ìƒì„± ì¤‘...")
    agent = Bamba(
        state_dim=dataset[0]['states'].shape[1],
        action_dim=dataset[0]['actions'].shape[0],
        hidden_dim=args.hidden_dim,
        context_length=args.context_length,
        d_model=args.d_model,
        d_state=args.d_state,
        d_conv=args.d_conv,
        expand=args.expand,
        tau=args.tau,
        alpha=args.alpha,
        gamma=args.gamma,
        action_noise=args.action_noise,
        action_noise_clip=args.action_noise_clip,
        n_layers=args.n_layers,
        device=device,
        warmup_steps=args.warmup_steps,
        policy_freq=args.policy_freq,
        drop_p=args.drop_p,
        beta_s=args.beta_s,
        beta_r=args.beta_r
    )
    
    # ì²´í¬í¬ì¸íŠ¸ì—ì„œ ë³µì›
    start_epoch = 0
    if args.resume:
        print(f"ğŸ”„ ì²´í¬í¬ì¸íŠ¸ì—ì„œ ë³µì› ì¤‘: {args.resume}")
        start_epoch, normalization_stats = load_checkpoint(
            agent, args.resume, device
        )
        start_epoch += 1
    
    # í›ˆë ¨ ë£¨í”„
    print("ğŸ¯ í›ˆë ¨ ì‹œì‘!")
    
    best_reward = float('-inf')
    training_logs = []
    
    for epoch in range(start_epoch, args.num_epochs):
        # í•œ ì—í¬í¬ í›ˆë ¨
        critic_loss, mamba_loss = train_epoch(agent, dataloader, device)
        
        # ë¡œê·¸ ê¸°ë¡
        log_entry = {
            'epoch': epoch,
            'critic_loss': critic_loss,
            'mamba_loss': mamba_loss,
            'timestamp': datetime.now().isoformat()
        }
        training_logs.append(log_entry)
        
        # 10 ì—í¬í¬ë§ˆë‹¤ êµ¬ë¶„ì„ ê³¼ í•¨ê»˜ ìƒì„¸ ë¡œê·¸ ì¶œë ¥
        if epoch % 10 == 0:
            current_lr = agent.get_current_lr()
            print("=" * 60)
            print(f"ğŸ“Š EPOCH {epoch:4d} ìƒì„¸ í›ˆë ¨ ê²°ê³¼")
            print(f"   Critic Loss: {critic_loss:.6f}")
            print(f"   Mamba Loss: {mamba_loss:.6f}")
            print(f"   Mamba LR: {current_lr['mamba_lr']:.2e}")
            print(f"   Critic LR: {current_lr['critic_lr']:.2e}")
            print("=" * 60)
        
        # í‰ê°€ (ì²« ë²ˆì§¸ ì—í¬í¬ ì œì™¸)
        if epoch > 0 and epoch % args.eval_freq == 0:
            print(f"ğŸ” Epoch {epoch} í‰ê°€ ì¤‘...")
            eval_results = evaluate_bamba_on_environment(
                model=agent,
                device=device,
                context_len=args.context_length,
                env_name=f"{args.env}-{args.dataset_type}-v2",
                num_eval_ep=5,
                max_test_ep_len=1000,
                state_mean=state_mean,
                state_std=state_std,
                render=False
            )
            
            if eval_results:
                avg_reward = eval_results.get('avg_reward', 0)
                d4rl_score = eval_results.get('d4rl_score', 0)
                log_entry['eval_avg_reward'] = avg_reward
                log_entry['eval_d4rl_score'] = d4rl_score
                print(f"ğŸ¯ í‰ê°€ ê²°ê³¼ - í‰ê·  ë³´ìƒ: {avg_reward:.2f} | D4RL ìŠ¤ì½”ì–´: {d4rl_score:.1f}/100")
                
                # ìµœê³  ì„±ëŠ¥ ì²´í¬í¬ì¸íŠ¸ ì €ì¥
                if avg_reward > best_reward:
                    best_reward = avg_reward
                    save_checkpoint(
                        agent, epoch,
                        normalization_stats, output_dir, "best_checkpoint.pt"
                    )
        
        # ì •ê¸° ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        if epoch % args.save_freq == 0:
            save_checkpoint(
                agent, epoch,
                normalization_stats, output_dir
            )
    
    # ìµœì¢… ì²´í¬í¬ì¸íŠ¸ ì €ì¥
    save_checkpoint(
        agent, args.num_epochs - 1,
        normalization_stats, output_dir, "final_checkpoint.pt"
    )
    
    # í›ˆë ¨ ë¡œê·¸ ì €ì¥
    with open(output_dir / 'training_logs.json', 'w') as f:
        json.dump(training_logs, f, indent=2)
    
    print("ğŸ‰ í›ˆë ¨ ì™„ë£Œ!")
    print(f"ğŸ“ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {output_dir}")

if __name__ == "__main__":
    main()
